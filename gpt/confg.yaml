Attention:
  n_embd: 128
  n_head: 8
  dropout: 0.1

Transformer_block:
  n_layer: 4

Training:
  batch_size: 32
  epochs: 10
  learning_rate: 0.001
  eval_iters: 500
  block_size: 128

File:
  file_path: "input.txt"

Model_save:
  model_save_path: "results"
  save_interval: 2